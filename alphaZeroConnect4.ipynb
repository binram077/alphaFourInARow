{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project AlphaFour**\n",
        "In this project I implemented the alphaGo algorithm depicted in this paper: https://augmentingcognition.com/assets/Silver2017a.pdf for Connect4, I also implemented a fast and efficient Connect4 environment that has integers as rows instead of arrays(though the bottleneck is not the environment but the neural net, but it was a fun challenge to optimize the environment).  "
      ],
      "metadata": {
        "id": "g0UGD_RC5PlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup"
      ],
      "metadata": {
        "id": "Cv_q2ppm5Kx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "imports"
      ],
      "metadata": {
        "id": "kTr3_cko5Ivy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from torch import nn, optim, stack, tensor, flatten\n",
        "from torch.nn import functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "YAh92Kfdo6FN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "configuration"
      ],
      "metadata": {
        "id": "xFx-kmlAUDLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"device\" : 'cpu',\n",
        "    \"mem_size\" : 1000,\n",
        "    \"mcts_iterations\" : 240,\n",
        "    'num_games_per_epoch' : 15,\n",
        "    'num_mcts_iterations_increase' : 20,\n",
        "    'batch_size' : 32,\n",
        "    'initial_tau' : 1.2,\n",
        "    'tau_multiplier' : 1.1,\n",
        "    'lr' : 3e-4,\n",
        "    'max_tau' : 2.0,\n",
        "    'num_training_steps_after_game' : 10,\n",
        "    'saving_dir' : 'models',\n",
        "    'num_blocks': 15,\n",
        "    'hidden_size': 200\n",
        "}"
      ],
      "metadata": {
        "id": "LPsQhR1DUE-2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "id": "An8uyL5raazG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The environment class saves each row as an integer where each square in the row takes two bits of the row (in total there are 14 bits occupied), if a quare is empty it would have \"00\" in it, if player one placed a piece in the square then it would have \"01\" in it and if player two played the square it would have \"10\". We can use the binary format to check for win efficiently etc. . The environment class also is responsible to provide the state translation to tensor."
      ],
      "metadata": {
        "id": "l2fArY3ZaeEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EffConnect4:\n",
        "    def reset():\n",
        "        board = [0 for _ in range(6)]\n",
        "        current_player = 1\n",
        "\n",
        "        return (board, current_player)\n",
        "\n",
        "    def get_row(state, row_number):\n",
        "      board, _ = state\n",
        "      row = board[row_number]\n",
        "      row = [row % 4, (row //4) % 4, (row //16) % 4, (row //64) % 4, (row //256) % 4, (row //1024) % 4, (row //4096) % 4 ]\n",
        "      return row\n",
        "\n",
        "    def check_win_cols(board):\n",
        "        lines_0_1, lines_1_2, lines_2_3, lines_3_4, lines_4_5 = \\\n",
        "         (board[0] & board[1]), (board[1] & board[2]), (board[2] & board[3])\\\n",
        "        ,(board[3] & board[4]), (board[4] & board[5])\n",
        "        res = (lines_0_1 & lines_2_3) | (lines_1_2 & lines_3_4) | (lines_2_3 & lines_4_5)\n",
        "        return bool(res & 10922) * 2 + bool(res & 5461) * 1\n",
        "\n",
        "    def check_win_ascending_diags(board):\n",
        "        lines_0_1, lines_1_2, lines_2_3, lines_3_4, lines_4_5 = \\\n",
        "         (board[0] << 10 & board[1] << 8) , (board[1] << 8 & board[2] << 6), (board[2] << 6 & board[3] << 4)\\\n",
        "        ,(board[3] << 4 & board[4] << 2), (board[4] << 2 & board[5])\n",
        "        res = (lines_0_1 & lines_2_3) | (lines_1_2 & lines_3_4) | (lines_2_3 & lines_4_5)\n",
        "        return bool(res & 11184810) * 2 + bool(res & 5592405) * 1\n",
        "\n",
        "    def check_win_descending_diags(board):\n",
        "        lines_0_1, lines_1_2, lines_2_3, lines_3_4, lines_4_5 = \\\n",
        "         (board[0] & board[1] << 2) , (board[1] << 2 & board[2] << 4), (board[2] << 4 & board[3] << 6)\\\n",
        "        ,(board[3] << 6 & board[4] << 8), (board[4] << 8 & board[5] << 10)\n",
        "        res = (lines_0_1 & lines_2_3) | (lines_1_2 & lines_3_4) | (lines_2_3 & lines_4_5)\n",
        "        return bool(res & 11184810) * 2 + bool(res & 5592405) * 1\n",
        "\n",
        "    def check_a_row(board, i):\n",
        "        return  2 * bool((board[i] & 10880 == 10880) | (board[i] & 2720 == 2720) | (board[i] & 680 == 680) | (board[i] & 170 == 170)) or\\\n",
        "                1 * bool((board[i] & 5440 == 5440) | (board[i] & 1360 == 1360) | (board[i] & 340 == 340) | (board[i] & 85 == 85))\n",
        "\n",
        "    def check_win_rows(board):\n",
        "        return EffConnect4.check_a_row(board, 0) | EffConnect4.check_a_row(board, 1) | EffConnect4.check_a_row(board, 2)\\\n",
        "              | EffConnect4.check_a_row(board, 3) | EffConnect4.check_a_row(board, 4) | EffConnect4.check_a_row(board, 5)\n",
        "\n",
        "    def check_win(state):\n",
        "        board, current_player = state\n",
        "        return EffConnect4.check_win_cols(board) | EffConnect4.check_win_rows(board) \\\n",
        "              | EffConnect4.check_win_descending_diags(board) | EffConnect4.check_win_ascending_diags(board)\n",
        "\n",
        "    def get_action_mask(board):\n",
        "        row = ~(board[-1] | (board[-1] >> 1)) & 5461\n",
        "        row = [row % 4, (row //4) % 4, (row //16) % 4, (row //64) % 4, (row //256) % 4, (row //1024) % 4, (row //4096) % 4 ]\n",
        "        return row\n",
        "\n",
        "    def get_legal_actions(state):\n",
        "        board, _ = state\n",
        "        mask = EffConnect4.get_action_mask(board)\n",
        "        return [i for i in range(7) if mask[i]]\n",
        "\n",
        "    def play_action(state, action):\n",
        "        board, current_player = state\n",
        "        board = list(board)\n",
        "        x = (current_player << 2 * action)\n",
        "        y = (3 << 2 * action)\n",
        "        for i in range(6):\n",
        "            if (board[i] & y):\n",
        "                continue\n",
        "            board[i] += x\n",
        "            current_player = (3 - current_player)\n",
        "            return (board, current_player), EffConnect4.get_action_mask(board), EffConnect4.is_terminal((board, current_player))\n",
        "        raise \"no action was played\"\n",
        "\n",
        "    def print_board(state):\n",
        "        board, current_player = state\n",
        "        print(f'Player {current_player} to play:')\n",
        "        print(\"board:\")\n",
        "        print(\"-\" * 20)\n",
        "        for i in range(6):\n",
        "          print(' , '.join(map(str, EffConnect4.get_row(state, 6 - i - 1))))\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    ''' translate the board and current player to one dimensional tensor '''\n",
        "    def states_to_tensor(states, device):\n",
        "        state_tensors = []\n",
        "        for state in states:\n",
        "            board, current_player = state\n",
        "            state_array = [current_player]\n",
        "            for i in range(len(board)):\n",
        "                state_array.extend(EffConnect4.get_row(state, i))\n",
        "            state_tensor = (tensor(state_array, dtype=torch.float32, device=device) - 1)/2\n",
        "            state_tensors.append(state_tensor)\n",
        "        return torch.stack(state_tensors)\n",
        "\n",
        "    ''' translate the mask of action to a tensor with -inf for unavilable actions '''\n",
        "    def action_masks_to_tensor(masks, device):\n",
        "        return torch.stack([torch.where(tensor(mask) == 1, torch.tensor(0.0), torch.tensor(float('-inf')))\n",
        "                            for mask in masks])\n",
        "\n",
        "    def is_full(state):\n",
        "        first_row = state[0][-1]\n",
        "        return ((first_row | (first_row >> 1)) & 5461) == 5461\n",
        "\n",
        "    def is_terminal(state):\n",
        "        return (EffConnect4.is_full(state) or (EffConnect4.check_win(state) != 0))\n",
        "\n",
        "    def get_value(state):\n",
        "        # 3 - current_player gives the other player\n",
        "        return (EffConnect4.check_win(state) == state[1]) - (EffConnect4.check_win(state) == (3 - state[1]))\n"
      ],
      "metadata": {
        "id": "D1TgUmJTFsLF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "t1 = time.time()\n",
        "env = EffConnect4\n",
        "state = env.reset()\n",
        "while not env.is_terminal(state):\n",
        "  env.print_board(state)\n",
        "  print(env.get_legal_actions(state))\n",
        "  action = random.choice(env.get_legal_actions(state))\n",
        "  state, _, _ = env.play_action(state, action)\n",
        "  print(f\"The action {action} was just played\")\n",
        "\n",
        "env.print_board(state)\n",
        "print(f\"value = {env.get_value(state)}\")\n",
        "print(time.time() - t1)\n",
        "\n",
        "  # print(f'is winning player {(i % 2) + 1}: {env.is_win((i % 2) + 1)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fWv7FntTA9-",
        "outputId": "2728be97-fa69-46f4-b9b1-b2f820bc1cde",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 0 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 4 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 4 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "1 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 5 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "1 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 2 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "1 , 0 , 1 , 0 , 2 , 2 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 4 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "1 , 0 , 1 , 0 , 2 , 2 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 0 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "1 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "1 , 0 , 1 , 0 , 2 , 2 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 4 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "1 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "1 , 0 , 1 , 0 , 2 , 2 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 5 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "1 , 0 , 0 , 0 , 1 , 1 , 0\n",
            "1 , 0 , 1 , 0 , 2 , 2 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 0 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "1 , 0 , 0 , 0 , 1 , 1 , 0\n",
            "1 , 0 , 1 , 0 , 2 , 2 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 1 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "1 , 0 , 0 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 0 , 2 , 2 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 5 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 0 , 0 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 0 , 2 , 2 , 0\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 6 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 0 , 0 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 0 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 2 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 0 , 2 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 0 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 1 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 0 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 0 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 0 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 5 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 1 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 0 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 0 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 1 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 0 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 4 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 1 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 0 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 3 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 1 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 2 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 1 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 1 , 0\n",
            "2 , 1 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 2 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 5 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 1 , 0\n",
            "2 , 1 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 2 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "The action 5 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 1 , 0\n",
            "2 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 1 , 0\n",
            "2 , 1 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 0 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 2 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 6]\n",
            "The action 3 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 1 , 0\n",
            "2 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 1 , 0\n",
            "2 , 1 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 2 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 2 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 4, 6]\n",
            "The action 4 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 1 , 1 , 0\n",
            "2 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 1 , 0\n",
            "2 , 1 , 0 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 2 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 2 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 6]\n",
            "The action 2 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 1 , 1 , 0\n",
            "2 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 1 , 0\n",
            "2 , 1 , 2 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 2 , 1 , 1 , 0\n",
            "1 , 1 , 1 , 2 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 6]\n",
            "The action 6 was just played\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 1 , 1 , 0\n",
            "2 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 1 , 0\n",
            "2 , 1 , 2 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 1 , 2 , 2 , 2 , 1\n",
            "--------------------\n",
            "[0, 1, 2, 3, 6]\n",
            "The action 0 was just played\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "2 , 0 , 0 , 0 , 1 , 1 , 0\n",
            "2 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "2 , 0 , 0 , 0 , 2 , 1 , 0\n",
            "2 , 1 , 2 , 0 , 2 , 2 , 0\n",
            "1 , 1 , 2 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 1 , 2 , 2 , 2 , 1\n",
            "--------------------\n",
            "value = -1\n",
            "0.05331707000732422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "SwF_BBEzveSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(in_features, out_features)\n",
        "        self.linear2 = nn.Linear(out_features, out_features)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(out_features)\n",
        "        self.norm2 = nn.LayerNorm(out_features)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.linear1(x)\n",
        "        out = self.norm1(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.activation(out)\n",
        "        return out + identity"
      ],
      "metadata": {
        "id": "-JK-8gOtUk3z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaFourModel(nn.Module):\n",
        "    def __init__(self, num_blocks, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.first_block = nn.Sequential(\n",
        "            nn.Linear(6*7 + 1, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.blocks = nn.ModuleList([LinearResidualBlock(hidden_dim, hidden_dim) for _ in range(num_blocks)])\n",
        "        self.policy_layers = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 500),\n",
        "            nn.LayerNorm(500),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(500, 7)\n",
        "        )\n",
        "        self.value_layers = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 500),\n",
        "            nn.LayerNorm(500),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(500, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, states, masks):\n",
        "        x = self.first_block(states)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        distributions = torch.softmax(self.policy_layers(x) + masks, dim = -1)\n",
        "        val = F.tanh(self.value_layers(x))\n",
        "        return val, distributions\n",
        "\n",
        "    def save(self, optimizer, file_name, version):\n",
        "        if not os.path.isdir(config['saving_dir']):\n",
        "            os.makedirs(config['saving_dir'])\n",
        "        print(f\"saving model into: {file_name} version: {version}\")\n",
        "        torch.save(self.state_dict(), os.path.join(config['saving_dir'], file_name + '_model_' + f'_version_{version}'))\n",
        "        torch.save(optimizer.state_dict(),  os.path.join(config['saving_dir'], file_name + '_optimizer_' + f'_version_{version}'))\n",
        "\n",
        "    def load(self, optimizer, file_name, version):\n",
        "        print(f\"loading model and optimizer from: {file_name} version: {version}\")\n",
        "        self.load_state_dict(torch.load(os.path.join(config['saving_dir'], file_name + '_model_' + f'_version_{version}')))\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(config['saving_dir'], file_name + '_optimizer_' + f'_version_{version}')))"
      ],
      "metadata": {
        "id": "p1WoT5jppV4d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlphaFourModel(config['num_blocks'], config['hidden_size']).to(config['device'])\n",
        "\n",
        "state = env.reset()\n",
        "for i in range(4):\n",
        "  action = random.choice(env.get_legal_actions(state))\n",
        "  state, _, _ = env.play_action(state, action)\n",
        "\n",
        "env.print_board(state)\n",
        "\n",
        "print(env.get_action_mask(state))\n",
        "\n",
        "print(model(env.states_to_tensor([state], config['device']), env.action_masks_to_tensor([env.get_action_mask(state)], config['device'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_ujGcJGPodY",
        "outputId": "4c8e0b5c-5887-4bfd-e5f9-0648a7868758"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 2 , 0 , 2 , 1 , 0 , 0\n",
            "--------------------\n",
            "[0, 1, 1, 1, 1, 1, 1]\n",
            "(tensor([[-0.0423]], grad_fn=<TanhBackward0>), tensor([[0.0000, 0.1368, 0.1312, 0.1445, 0.1191, 0.1077, 0.3608]],\n",
            "       grad_fn=<SoftmaxBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MCTS"
      ],
      "metadata": {
        "id": "XKDlapxsaZJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " here I created a monte carlo tree search that get a model, an environment, and adapter that process the data from the environment and pass it to the model.\n",
        " It will need to get a state and then run num_simulations of simulations using the model and then it'll return the policy as calculated by the mcts algorithm of the alphazero-paper.https://augmentingcognition.com/assets/Silver2017a.pdf"
      ],
      "metadata": {
        "id": "6I5_BoucdESg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MCTS search as a function gets an MCTS node that can be an empty root node but can also be a partially filled node, it than runs num_steps extension steps and the proccess of an extension step goes as follow:\n",
        "start at root as current node:\n",
        "1.if current node is terminal:\n",
        "    backprop its value back\n",
        "2.for each valid next action:\n",
        "    if action was already played ask the son node for its ucb\n",
        "    else regard the son node value as zero and"
      ],
      "metadata": {
        "id": "nYpgKjzPVga5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_n_simulations(root, num_simulations, env, model):\n",
        "    if env.is_terminal(root.state):\n",
        "        return\n",
        "    for i in range(num_simulations):\n",
        "        run_simulation(root, env, model)"
      ],
      "metadata": {
        "id": "UbljvYikVYhh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_simulation(root, env, model):\n",
        "    current_node = root\n",
        "    visited_edges = []\n",
        "    while True:\n",
        "        sum_actions_played = current_node.visit_count()\n",
        "        if not current_node.edges:\n",
        "            print(current_node.__dict__)\n",
        "        edge = max(current_node.edges, key=lambda e: e.uct(sum_actions_played))\n",
        "        visited_edges.insert(0, edge)\n",
        "        # when we get the value of the node after the edge we need to multiply it by -1\n",
        "        # because the edge is played by the other player and we want to measure\n",
        "        # Q(last_state(viewed from the former player), action)\n",
        "        if edge.visit_count == 0:\n",
        "            new_node = edge.open_edge(current_node.state, env, model)\n",
        "            backprop(val = -1 * new_node.value, edges = visited_edges)\n",
        "            break\n",
        "        current_node = edge.node_after\n",
        "        if env.is_terminal(current_node.state):\n",
        "            backprop(val = -1 * current_node.value, edges = visited_edges)\n",
        "            break"
      ],
      "metadata": {
        "id": "fktJrc7v2gN8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop(val, edges):\n",
        "    for edge in edges:\n",
        "        edge.visit_count += 1\n",
        "        edge.Q += val\n",
        "        val *= -1"
      ],
      "metadata": {
        "id": "U8MAX7p94VfK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MCTSEdge():\n",
        "    def __init__(self, action, probability):\n",
        "        self.action = action\n",
        "        self.probability = probability\n",
        "        self.visit_count = 0\n",
        "        self.Q = 0\n",
        "        self.node_after = None\n",
        "\n",
        "    def open_edge(self, last_state, env, model):\n",
        "        if self.node_after:\n",
        "            raise f\"edge was already open:\\nstate: {state},\\naction: {self.action}\\nstate_after: {self.state_after}\"\n",
        "        new_state, new_state_action_mask, is_terminal = env.play_action(last_state, self.action)\n",
        "        self.node_after = MCTSNode(new_state, new_state_action_mask, is_terminal, env, model)\n",
        "\n",
        "        return self.node_after\n",
        "\n",
        "    def uct(self, sum_action_played_from_father_state ,c_puct = 1):\n",
        "        U = c_puct * self.probability * (((sum_action_played_from_father_state + 1) ** 0.5) / (1 + self.visit_count))\n",
        "        Q = self.Q / (self.visit_count + 1)\n",
        "        return Q + U\n",
        ""
      ],
      "metadata": {
        "id": "FelUq5wN6402"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MCTSNode():\n",
        "    def __init__(self, state, action_mask, is_terminal, env, model, device = config[\"device\"]):\n",
        "        self.state = state\n",
        "        self.action_mask = action_mask\n",
        "        self.edges = []\n",
        "        self.value = 0\n",
        "\n",
        "        if is_terminal:\n",
        "            self.init_terminal(env)\n",
        "        else:\n",
        "            self.init_non_terminal(state, action_mask, env, model, device)\n",
        "\n",
        "\n",
        "    def init_non_terminal(self, new_state, new_state_action_mask, env , model, device):\n",
        "        new_state_tensor = env.states_to_tensor([new_state], device=device)\n",
        "        new_state_action_mask_tensor = env.action_masks_to_tensor([new_state_action_mask], device=device)\n",
        "\n",
        "        value, probs = model(new_state_tensor, new_state_action_mask_tensor)\n",
        "\n",
        "        self.value = value.cpu().item()\n",
        "\n",
        "        for i in range(len(new_state_action_mask)):\n",
        "            if new_state_action_mask[i]:\n",
        "                self.edges.append(MCTSEdge(i, probs[0][i].cpu().item()))\n",
        "        if not self.edges:\n",
        "            raise f\"this state do not possess any actions, and should be terminal or possess actions\\n{new_state_tensor.__dict__}\"\n",
        "\n",
        "    def init_terminal(self, env):\n",
        "        self.value = env.get_value(self.state)\n",
        "\n",
        "    def visit_count(self):\n",
        "        return sum([edge.visit_count for edge in self.edges])\n",
        "\n",
        "    def extract_policy(self, tau):\n",
        "        action_count = [0] * len(self.action_mask) # we create a list in the size of action_space\n",
        "        for edge in self.edges:\n",
        "            action_count[edge.action] = edge.visit_count ** tau\n",
        "\n",
        "        policy_tensor = tensor(action_count)\n",
        "        policy_tensor = policy_tensor / sum(policy_tensor)\n",
        "        return policy_tensor"
      ],
      "metadata": {
        "id": "DVfDWORP2jfs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's try to make a node"
      ],
      "metadata": {
        "id": "qmmxrvgA8ACD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "t1 = time.time()\n",
        "env = EffConnect4\n",
        "model = AlphaFourModel(config['num_blocks'], config['hidden_size']).to(config['device'])\n",
        "root_state = env.reset()\n",
        "\n",
        "root_node = MCTSNode(root_state, env.get_action_mask(state[0]), False, env, model)\n",
        "print(root_node.edges[0].uct(0))\n",
        "print([e.probability for e in root_node.edges])\n",
        "\n",
        "run_n_simulations(root_node, 20, env, model)\n",
        "\n",
        "for edge in root_node.edges:\n",
        "    print(edge.__dict__)\n",
        "\n",
        "print(root_node.extract_policy(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KTxtkV87_zK",
        "outputId": "991b4a25-f3b1-446b-f632-030fb680ca07"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13138513267040253\n",
            "[0.13138513267040253, 0.06733889877796173, 0.13779036700725555, 0.19699271023273468, 0.12018755823373795, 0.1973654329776764, 0.14893987774848938]\n",
            "{'action': 0, 'probability': 0.13138513267040253, 'visit_count': 1, 'Q': -0.5141957402229309, 'node_after': <__main__.MCTSNode object at 0x7a927453c760>}\n",
            "{'action': 1, 'probability': 0.06733889877796173, 'visit_count': 1, 'Q': -0.5036095380783081, 'node_after': <__main__.MCTSNode object at 0x7a927455cca0>}\n",
            "{'action': 2, 'probability': 0.13779036700725555, 'visit_count': 1, 'Q': -0.4948214292526245, 'node_after': <__main__.MCTSNode object at 0x7a927453f040>}\n",
            "{'action': 3, 'probability': 0.19699271023273468, 'visit_count': 3, 'Q': 0.5038596093654633, 'node_after': <__main__.MCTSNode object at 0x7a92745470a0>}\n",
            "{'action': 4, 'probability': 0.12018755823373795, 'visit_count': 1, 'Q': -0.503229558467865, 'node_after': <__main__.MCTSNode object at 0x7a927453ca90>}\n",
            "{'action': 5, 'probability': 0.1973654329776764, 'visit_count': 12, 'Q': 1.0771270394325256, 'node_after': <__main__.MCTSNode object at 0x7a9274545e70>}\n",
            "{'action': 6, 'probability': 0.14893987774848938, 'visit_count': 1, 'Q': -0.508668839931488, 'node_after': <__main__.MCTSNode object at 0x7a9274546770>}\n",
            "tensor([0.0063, 0.0063, 0.0063, 0.0570, 0.0063, 0.9114, 0.0063])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replay Buffer and terminal states Replay Buffer"
      ],
      "metadata": {
        "id": "U20hWCk6bNZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, mem_size, state_shape, n_actions):\n",
        "        self.mem_counter = 0\n",
        "        self.max_size = mem_size\n",
        "        self.states = torch.zeros((mem_size, *state_shape), dtype = torch.float32)\n",
        "        self.policies = torch.zeros((mem_size, n_actions), dtype = torch.float32)\n",
        "        self.values = torch.zeros((mem_size, 1), dtype = torch.float32)\n",
        "        self.masks = torch.zeros((mem_size, n_actions), dtype = torch.float32)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        max_index = min(self.mem_counter, self.max_size)\n",
        "        indexes = np.array(random.sample(range(max_index), min(batch_size, max_index)), dtype = np.intc)\n",
        "\n",
        "        return self.states[indexes], self.policies[indexes], self.values[indexes], self.masks[indexes]\n",
        "\n",
        "    def enter(self, states, policies, values, masks):\n",
        "        for i in range(len(states)):\n",
        "            self.states[self.mem_counter % self.max_size] = states[i]\n",
        "            self.policies[self.mem_counter % self.max_size] = policies[i]\n",
        "            self.values[self.mem_counter % self.max_size] = values[i]\n",
        "            self.masks[self.mem_counter % self.max_size] = masks[i]\n",
        "            self.mem_counter += 1"
      ],
      "metadata": {
        "id": "jK2m5Tqq1N7W"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Game playing function"
      ],
      "metadata": {
        "id": "PDGq3qWu4Mi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_edges_except(mcts_node, action):\n",
        "    mcts_node.edges = [e for e in mcts_node.edges if e.action == action]"
      ],
      "metadata": {
        "id": "ZmBHTMs7C2P9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_states_masks_policy_value_to_replay_buffer(state_policy_value, replay_buffer, env, device=config['device']):\n",
        "    states, policies, values = list(zip(*state_policy_value))\n",
        "    policies_tensor = torch.stack(policies)\n",
        "    values_tensor = torch.stack(values)\n",
        "    states_tensor = env.states_to_tensor(states, device)\n",
        "    masks_tensor = env.action_masks_to_tensor([env.get_action_mask(state) for state in states], device)\n",
        "\n",
        "    replay_buffer.enter(states_tensor, policies_tensor, values_tensor, masks_tensor)"
      ],
      "metadata": {
        "id": "hVI5va8_nW3F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_game(num_simulations_per_turn, tau, env, model, replay_buffer, show_game = False):\n",
        "    state_policy_value = []\n",
        "\n",
        "    root_state = env.reset()\n",
        "    current_node = MCTSNode(root_state, env.get_action_mask(state[0]), False, env, model)\n",
        "\n",
        "    while not env.is_terminal(current_node.state):\n",
        "        if show_game:\n",
        "            print(\"first the state was:\")\n",
        "            env.print_board(current_node.state)\n",
        "            print(env.get_action_mask(current_node.state[0]))\n",
        "        run_n_simulations(current_node, num_simulations_per_turn - current_node.visit_count(), env, model)\n",
        "        policy_dist = current_node.extract_policy(tau)\n",
        "        action = torch.multinomial(policy_dist, num_samples=1).item()\n",
        "\n",
        "        if show_game: print(f\"action ditribution is {policy_dist}\")\n",
        "        if show_game: print(f\"then action {action} was taken\")\n",
        "\n",
        "        # this is in order that the python garbage collector will collect the other edges\n",
        "        delete_edges_except(current_node, action)\n",
        "\n",
        "        state_policy_value.insert(0, [current_node.state, policy_dist, None])\n",
        "\n",
        "        edge = next((edge for edge in current_node.edges if edge.action == action), -1)\n",
        "        current_node = edge.node_after\n",
        "\n",
        "    val = -1 * env.get_value(current_node.state)\n",
        "    for i in range(len(state_policy_value)):\n",
        "        state_policy_value[i][2] = tensor(val)\n",
        "        val *= -1\n",
        "\n",
        "    write_states_masks_policy_value_to_replay_buffer(state_policy_value, replay_buffer, env)\n",
        "\n",
        "    return current_node\n",
        "\n"
      ],
      "metadata": {
        "id": "7gIqcZM64L73"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = EffConnect4\n",
        "model = AlphaFourModel(config['num_blocks'], config['hidden_size']).to(config['device'])\n",
        "rb = ReplayBuffer(1000, state_shape=(43,), n_actions=7)\n",
        "\n",
        "end_node = play_game(20, 2, env, model, rb, True)\n",
        "\n",
        "env.print_board(end_node.state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xcg7qvhXHjRZ",
        "outputId": "2ab29f85-0962-4c4d-91fd-f42564243ec1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0056, 0.0056, 0.0056, 0.0056, 0.9494, 0.0225, 0.0056])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0056, 0.0056, 0.0056, 0.0056, 0.9494, 0.0225, 0.0056])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0056, 0.0056, 0.0056, 0.0056, 0.9494, 0.0225, 0.0056])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0063, 0.0063, 0.0063, 0.0063, 0.9114, 0.0570, 0.0063])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0056, 0.0056, 0.0056, 0.0056, 0.9494, 0.0225, 0.0056])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0056, 0.0056, 0.0056, 0.0056, 0.9494, 0.0225, 0.0056])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0063, 0.0063, 0.0063, 0.0063, 0.9114, 0.0570, 0.0063])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 0, 1, 1]\n",
            "action ditribution is tensor([0.0061, 0.0061, 0.0976, 0.0061, 0.0000, 0.8780, 0.0061])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 0, 1, 1]\n",
            "action ditribution is tensor([0.0061, 0.0061, 0.0061, 0.0061, 0.0000, 0.8780, 0.0976])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 0, 1, 1]\n",
            "action ditribution is tensor([0.0061, 0.0061, 0.0061, 0.0061, 0.0000, 0.8780, 0.0976])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 0, 1, 1]\n",
            "action ditribution is tensor([0.0056, 0.0056, 0.0222, 0.0056, 0.0000, 0.9389, 0.0222])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "0 , 1 , 0 , 0 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 0, 1, 1]\n",
            "action ditribution is tensor([0., 0., 0., 0., 0., 1., 0.])\n",
            "then action 5 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "0 , 0 , 0 , 0 , 2 , 2 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 2 , 0\n",
            "0 , 1 , 0 , 0 , 1 , 2 , 1\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "uhcEQZCR7fw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_from_batch(model, optimizer, batch):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    states, policies, values, masks = batch\n",
        "    values_pred, probs_pred = model(states, masks)\n",
        "\n",
        "    policy_loss = torch.nn.functional.kl_div(probs_pred, policies)\n",
        "\n",
        "    value_loss = torch.nn.functional.mse_loss(values_pred, values)\n",
        "\n",
        "    l2_reg = torch.tensor(0.)\n",
        "    for param in model.parameters():\n",
        "        l2_reg += torch.norm(param, 2)\n",
        "\n",
        "    loss = 0.1 * l2_reg - policy_loss + value_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "npFC-DQIBEyd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(num_epochs, env, rb, model, optimizer):\n",
        "    batch_size = config['batch_size']\n",
        "    num_games_per_epoch = config['num_games_per_epoch']\n",
        "    num_simulations_per_turn = config['mcts_iterations']\n",
        "    num_training_steps_after_game = config['num_training_steps_after_game']\n",
        "    tau = config['initial_tau']\n",
        "    for i in range(num_epochs):\n",
        "        for j in range(num_games_per_epoch):\n",
        "            if j == 0:\n",
        "                print(f\"Game No.{i}\")\n",
        "                print(3*(20*\"-\" + \"\\n\"))\n",
        "                play_game(num_simulations_per_turn, tau, env, model, rb, True)\n",
        "            play_game(num_simulations_per_turn, tau, env, model, rb)\n",
        "\n",
        "            for _ in range(num_training_steps_after_game):\n",
        "                batch = rb.sample(batch_size)\n",
        "                learn_from_batch(model, optimizer, batch)\n",
        "\n",
        "        tau = min(tau * config['tau_multiplier'], config['max_tau'])\n",
        "        num_simulations_per_turn += config['num_mcts_iterations_increase']\n",
        "\n",
        "        model.save(optimizer=optimizer, file_name=\"alpha_four\", version=i)\n"
      ],
      "metadata": {
        "id": "I7_Ss01j7XNb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = EffConnect4\n",
        "rb = ReplayBuffer(1000, state_shape=(43,), n_actions=7)\n",
        "\n",
        "model = AlphaFourModel(config['num_blocks'], config['hidden_size']).to(config['device'])\n",
        "optimizer = optim.Adam(model.parameters(), lr = config['lr'])"
      ],
      "metadata": {
        "id": "vXwZ1kfuJmzB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(12, env, rb, model, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aNFwJ6-KyWn",
        "outputId": "c53f5daa-e0f0-48af-992e-50bb1dd13e47"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Game No.0\n",
            "--------------------\n",
            "--------------------\n",
            "--------------------\n",
            "\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.2068, 0.0969, 0.1016, 0.1755, 0.1207, 0.2015, 0.0969])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.2177, 0.1018, 0.0971, 0.1656, 0.1257, 0.1809, 0.1112])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.2073, 0.0925, 0.1113, 0.1759, 0.1209, 0.1759, 0.1161])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 2 , 0 , 0 , 0 , 0 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.2021, 0.0972, 0.1066, 0.1556, 0.1456, 0.1864, 0.1066])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 2 , 0 , 0 , 0 , 0 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1655, 0.0924, 0.0970, 0.2017, 0.1256, 0.2017, 0.1160])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 0 , 0 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1245, 0.3083, 0.0870, 0.1540, 0.1055, 0.1245, 0.0962])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 0 , 0 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0634, 0.0439, 0.0439, 0.0634, 0.6193, 0.1184, 0.0477])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 0 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0078, 0.0034, 0.0034, 0.0078, 0.0055, 0.9666, 0.0055])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1711, 0.1068, 0.1115, 0.1971, 0.1260, 0.1711, 0.1163])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1762, 0.1067, 0.1067, 0.1970, 0.1211, 0.1711, 0.1211])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1708, 0.1019, 0.1066, 0.2020, 0.1161, 0.1863, 0.1161])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 0 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.6722, 0.0432, 0.0469, 0.0702, 0.0545, 0.0662, 0.0469])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0664, 0.0432, 0.0470, 0.6679, 0.0546, 0.0664, 0.0546])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.6901, 0.0428, 0.0428, 0.0657, 0.0465, 0.0657, 0.0465])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1811, 0.1307, 0.0972, 0.1915, 0.1066, 0.1863, 0.1066])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.6857, 0.0429, 0.0466, 0.0658, 0.0466, 0.0658, 0.0466])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 1 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.9392, 0.0105, 0.0081, 0.0131, 0.0105, 0.0105, 0.0081])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 1 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1652, 0.1110, 0.1062, 0.2066, 0.0969, 0.2172, 0.0969])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 1 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 0 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0533, 0.0422, 0.0422, 0.0533, 0.0459, 0.7209, 0.0422])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 1 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 1 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0135, 0.0163, 0.0163, 0.0135, 0.9049, 0.0191, 0.0163])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 1 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 2 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 1 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1917, 0.1659, 0.1020, 0.1608, 0.1067, 0.1710, 0.1020])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 1 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 2 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 1 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0299, 0.8207, 0.0299, 0.0266, 0.0299, 0.0332, 0.0299])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 1 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 2 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 1 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 0, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.3834, 0.0000, 0.0878, 0.1428, 0.0922, 0.2017, 0.0922])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "1 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 1 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 2 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 1 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[0, 0, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0000, 0.0000, 0.0254, 0.0413, 0.8636, 0.0413, 0.0285])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "1 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "2 , 1 , 0 , 1 , 2 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 2 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 1 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[0, 0, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0000, 0.0000, 0.0468, 0.0539, 0.7805, 0.0686, 0.0503])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "1 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 1 , 0 , 2 , 1 , 0 , 0\n",
            "2 , 1 , 0 , 1 , 2 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 2 , 0 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 1 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[0, 0, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0000, 0.0000, 0.0217, 0.0338, 0.0189, 0.9038, 0.0217])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "1 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "1 , 1 , 0 , 2 , 1 , 0 , 0\n",
            "2 , 1 , 0 , 1 , 2 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 2 , 2 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 1 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[0, 0, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0000, 0.0000, 0.1748, 0.2502, 0.1554, 0.2399, 0.1796])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "1 , 2 , 0 , 1 , 0 , 0 , 0\n",
            "1 , 1 , 0 , 2 , 1 , 0 , 0\n",
            "2 , 1 , 0 , 1 , 2 , 0 , 0\n",
            "2 , 1 , 0 , 2 , 2 , 2 , 0\n",
            "2 , 2 , 0 , 1 , 2 , 1 , 0\n",
            "1 , 2 , 0 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[0, 0, 1, 0, 1, 1, 1]\n",
            "action ditribution is tensor([0.0000, 0.0000, 0.0033, 0.0000, 0.0076, 0.0124, 0.9766])\n",
            "then action 6 was taken\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0142, 0.8501, 0.0547, 0.0177, 0.0273, 0.0258, 0.0102])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([3.9642e-04, 6.5531e-04, 9.9672e-01, 6.5531e-04, 5.1777e-04, 3.9642e-04,\n",
            "        6.5531e-04])\n",
            "then action 2 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 2 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([3.6821e-03, 4.5458e-03, 9.7562e-01, 5.5004e-03, 1.6365e-03, 8.2847e-03,\n",
            "        7.2732e-04])\n",
            "then action 2 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 2 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([8.1060e-04, 9.3705e-01, 8.1060e-04, 6.2061e-04, 2.5648e-02, 8.1060e-04,\n",
            "        3.4248e-02])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 2 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0029, 0.0016, 0.9775, 0.0088, 0.0022, 0.0045, 0.0025])\n",
            "then action 2 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 2 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0052, 0.3814, 0.0026, 0.0026, 0.1412, 0.4007, 0.0662])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 2 , 0 , 0 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([1.8825e-03, 8.6442e-05, 9.7738e-01, 1.8595e-02, 1.8825e-03, 8.6442e-05,\n",
            "        8.6442e-05])\n",
            "then action 2 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 2 , 0 , 0 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0035, 0.0027, 0.9829, 0.0039, 0.0018, 0.0021, 0.0031])\n",
            "then action 2 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 2 , 0 , 0 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.2829, 0.0500, 0.0193, 0.3092, 0.1842, 0.0398, 0.1146])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "1 , 1 , 2 , 0 , 0 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([1.7764e-04, 9.9920e-01, 6.3949e-05, 1.7764e-04, 6.3949e-05, 6.3949e-05,\n",
            "        2.5579e-04])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "1 , 1 , 2 , 0 , 0 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0549, 0.0166, 0.0858, 0.4781, 0.2309, 0.0759, 0.0577])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "1 , 1 , 2 , 1 , 0 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([2.6840e-05, 6.0391e-05, 6.0391e-05, 9.9977e-01, 2.6840e-05, 2.6840e-05,\n",
            "        2.6840e-05])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 0 , 0 , 0\n",
            "1 , 1 , 2 , 1 , 0 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0502, 0.0530, 0.0450, 0.1850, 0.4360, 0.1598, 0.0710])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 0 , 0 , 0\n",
            "1 , 1 , 2 , 1 , 0 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([1.1537e-01, 8.7913e-01, 3.6179e-03, 1.2519e-03, 3.1297e-04, 1.1267e-04,\n",
            "        2.0030e-04])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 0 , 0 , 0\n",
            "1 , 1 , 2 , 1 , 0 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0788, 0.1276, 0.1101, 0.0861, 0.3597, 0.1276, 0.1101])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 0 , 0 , 0\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([4.5735e-01, 7.1460e-03, 6.4314e-02, 3.8020e-01, 3.9563e-04, 1.5825e-03,\n",
            "        8.9016e-02])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 0 , 0 , 0\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0437, 0.0437, 0.0595, 0.4538, 0.3044, 0.0462, 0.0487])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([4.6667e-01, 6.1929e-02, 3.2595e-02, 3.8095e-04, 1.4881e-02, 2.3810e-03,\n",
            "        4.2117e-01])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 0 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0572, 0.0940, 0.0940, 0.2116, 0.4227, 0.0602, 0.0602])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([6.1653e-05, 6.1653e-05, 6.1653e-05, 6.1653e-05, 9.9963e-01, 6.1653e-05,\n",
            "        6.1653e-05])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 0 , 2 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1103, 0.1282, 0.1191, 0.1146, 0.1236, 0.1191, 0.2851])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 0 , 2 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0393, 0.0291, 0.0273, 0.3665, 0.0330, 0.4657, 0.0393])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 0 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 0 , 2 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0229, 0.0229, 0.0229, 0.0858, 0.0563, 0.7630, 0.0261])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 0 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 0 , 2 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([6.6635e-04, 5.2650e-04, 5.2650e-04, 9.9627e-01, 5.2650e-04, 6.6635e-04,\n",
            "        8.2265e-04])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 0 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 2 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0952, 0.0704, 0.0704, 0.4609, 0.0915, 0.1238, 0.0878])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 2 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([2.3224e-01, 2.1205e-01, 1.7444e-01, 2.5804e-04, 4.5874e-04, 3.5965e-01,\n",
            "        2.0901e-02])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 2 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.2213, 0.1199, 0.1694, 0.0984, 0.1067, 0.1859, 0.0984])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 2 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0036, 0.0081, 0.0087, 0.0087, 0.0045, 0.9529, 0.0134])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 2 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0745, 0.0713, 0.0713, 0.0713, 0.0778, 0.0745, 0.5592])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 2 , 0 , 0\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([1.1250e-04, 1.1250e-04, 1.1250e-04, 1.1250e-04, 6.3283e-05, 1.1250e-04,\n",
            "        9.9937e-01])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 2 , 0 , 2\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1476, 0.0768, 0.0803, 0.0768, 0.0803, 0.3809, 0.1574])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 2 , 1 , 2\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0109, 0.3580, 0.1949, 0.2981, 0.0246, 0.0595, 0.0540])\n",
            "then action 2 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 2 , 1 , 2\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 0, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1966, 0.1607, 0.0000, 0.0922, 0.2187, 0.1807, 0.1511])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 2 , 0 , 1 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 2 , 1 , 2\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 0, 1, 0, 1, 1]\n",
            "action ditribution is tensor([0.0018, 0.0918, 0.0000, 0.6205, 0.0000, 0.2129, 0.0731])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 2 , 2 , 1 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 2 , 1 , 2\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 0, 0, 0, 1, 1]\n",
            "action ditribution is tensor([0.2335, 0.2017, 0.0000, 0.0000, 0.0000, 0.4343, 0.1305])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 2 , 2 , 1 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 0 , 0\n",
            "1 , 1 , 1 , 2 , 2 , 1 , 2\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 1, 0, 0, 0, 1, 1]\n",
            "action ditribution is tensor([0.0929, 0.4638, 0.0000, 0.0000, 0.0000, 0.3158, 0.1276])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 2 , 2 , 2 , 1 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 0 , 0\n",
            "1 , 1 , 1 , 2 , 2 , 1 , 2\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 0, 0, 0, 0, 1, 1]\n",
            "action ditribution is tensor([0.1518, 0.0000, 0.0000, 0.0000, 0.0000, 0.5561, 0.2921])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 2 , 2 , 2 , 1 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 1 , 0\n",
            "1 , 1 , 1 , 2 , 2 , 1 , 2\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 0, 0, 0, 0, 1, 1]\n",
            "action ditribution is tensor([0.0080, 0.0000, 0.0000, 0.0000, 0.0000, 0.8638, 0.1282])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 1 , 0\n",
            "1 , 1 , 1 , 2 , 2 , 1 , 2\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 0, 0, 0, 0, 0, 1]\n",
            "action ditribution is tensor([0.4950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5050])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "0 , 2 , 2 , 1 , 2 , 1 , 1\n",
            "1 , 1 , 1 , 2 , 2 , 1 , 2\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 0, 0, 0, 0, 0, 1]\n",
            "action ditribution is tensor([0.0011, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9989])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 2 , 2 , 2 , 1 , 2 , 2\n",
            "0 , 2 , 2 , 1 , 2 , 1 , 1\n",
            "1 , 1 , 1 , 2 , 2 , 1 , 2\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 0, 0, 0, 0, 0, 0]\n",
            "action ditribution is tensor([1., 0., 0., 0., 0., 0., 0.])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 2 , 2 , 2 , 1 , 2 , 2\n",
            "1 , 2 , 2 , 1 , 2 , 1 , 1\n",
            "1 , 1 , 1 , 2 , 2 , 1 , 2\n",
            "2 , 2 , 1 , 2 , 1 , 2 , 1\n",
            "2 , 2 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 1 , 2 , 1 , 1 , 2 , 1\n",
            "--------------------\n",
            "[1, 0, 0, 0, 0, 0, 0]\n",
            "action ditribution is tensor([1., 0., 0., 0., 0., 0., 0.])\n",
            "then action 0 was taken\n",
            "saving model into: alpha_four version: 8\n",
            "Game No.9\n",
            "--------------------\n",
            "--------------------\n",
            "--------------------\n",
            "\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1020, 0.0453, 0.0363, 0.1909, 0.4595, 0.1494, 0.0166])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([6.3234e-04, 5.3705e-01, 4.6458e-04, 4.6458e-04, 2.0648e-04, 2.0648e-04,\n",
            "        4.6098e-01])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0114, 0.0369, 0.0114, 0.0047, 0.1082, 0.8201, 0.0073])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 1 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([1.3161e-04, 4.0305e-04, 4.0305e-04, 1.9750e-02, 1.3161e-04, 1.3161e-04,\n",
            "        9.7905e-01])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0212, 0.0244, 0.5386, 0.0391, 0.2764, 0.0790, 0.0212])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([1.6757e-04, 2.4130e-04, 9.9870e-01, 2.4130e-04, 2.4130e-04, 1.6757e-04,\n",
            "        2.4130e-04])\n",
            "then action 2 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0125, 0.0101, 0.0133, 0.9388, 0.0133, 0.0073, 0.0045])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([2.7913e-04, 4.1888e-02, 5.6682e-02, 1.1165e-03, 2.9484e-03, 9.8133e-02,\n",
            "        7.9895e-01])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0014, 0.0014, 0.0014, 0.9914, 0.0014, 0.0014, 0.0014])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([1.0197e-04, 1.0197e-04, 1.0197e-04, 9.9939e-01, 1.0197e-04, 1.0197e-04,\n",
            "        1.0197e-04])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0563, 0.0039, 0.0050, 0.1165, 0.8087, 0.0056, 0.0039])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([2.4500e-05, 9.9969e-01, 5.5125e-05, 2.4500e-05, 2.4500e-05, 2.4500e-05,\n",
            "        1.5312e-04])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0092, 0.9442, 0.0085, 0.0139, 0.0106, 0.0072, 0.0066])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([1.0651e-04, 2.7702e-01, 2.9585e-04, 1.0651e-04, 1.8934e-04, 2.9585e-04,\n",
            "        7.2198e-01])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0328, 0.0400, 0.0363, 0.0262, 0.0345, 0.0363, 0.7938])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 1\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([2.6007e-05, 9.9908e-01, 2.3406e-04, 1.0403e-04, 2.3406e-04, 1.6254e-04,\n",
            "        1.6254e-04])\n",
            "then action 1 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 2 , 0 , 0 , 1\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1339, 0.1250, 0.1675, 0.0711, 0.2523, 0.1339, 0.1163])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 2 , 0 , 0 , 1\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 1 , 1 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1262, 0.1817, 0.1435, 0.0948, 0.2356, 0.0050, 0.2132])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 2 , 0 , 0 , 1\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 1 , 1 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([3.3181e-04, 2.4378e-04, 1.6929e-04, 2.4378e-04, 3.3181e-04, 9.9851e-01,\n",
            "        1.6929e-04])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 2 , 0 , 0 , 1\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 1 , 1 , 1 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0542, 0.2281, 0.2169, 0.0437, 0.0821, 0.2823, 0.0927])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 2\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 2 , 0 , 0 , 1\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 1 , 1 , 1 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 1 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 0]\n",
            "action ditribution is tensor([4.7121e-05, 4.7121e-05, 4.9991e-01, 4.7121e-05, 4.7121e-05, 4.9991e-01,\n",
            "        0.0000e+00])\n",
            "then action 5 was taken\n",
            "saving model into: alpha_four version: 9\n",
            "Game No.10\n",
            "--------------------\n",
            "--------------------\n",
            "--------------------\n",
            "\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0109, 0.0070, 0.0225, 0.0146, 0.7814, 0.1536, 0.0100])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([2.9799e-03, 9.0141e-04, 1.0728e-03, 1.0728e-03, 7.4497e-04, 7.4497e-04,\n",
            "        9.9248e-01])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([4.8457e-03, 8.9003e-04, 2.4273e-02, 9.4976e-01, 9.8892e-03, 9.2409e-03,\n",
            "        1.0988e-03])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 1 , 0 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0162, 0.0060, 0.0021, 0.0190, 0.0144, 0.0060, 0.9363])\n",
            "then action 6 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 2\n",
            "0 , 0 , 0 , 1 , 1 , 0 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([3.5972e-04, 1.6783e-01, 7.4799e-01, 9.2089e-04, 5.1800e-04, 8.0938e-02,\n",
            "        1.4389e-03])\n",
            "then action 2 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 2\n",
            "0 , 0 , 1 , 1 , 1 , 0 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0805, 0.2802, 0.0551, 0.0603, 0.0686, 0.4146, 0.0408])\n",
            "then action 5 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 2\n",
            "0 , 0 , 1 , 1 , 1 , 2 , 2\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([2.1833e-05, 9.9987e-01, 2.1833e-05, 2.1833e-05, 2.1833e-05, 2.1833e-05,\n",
            "        2.1833e-05])\n",
            "then action 1 was taken\n",
            "saving model into: alpha_four version: 10\n",
            "Game No.11\n",
            "--------------------\n",
            "--------------------\n",
            "--------------------\n",
            "\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.0112, 0.0030, 0.0026, 0.7965, 0.1187, 0.0533, 0.0147])\n",
            "then action 3 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([9.9100e-01, 4.6638e-03, 5.5883e-04, 8.3479e-04, 5.5883e-04, 8.3479e-04,\n",
            "        1.5523e-03])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.4189, 0.0044, 0.0115, 0.0533, 0.5027, 0.0049, 0.0044])\n",
            "then action 4 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 1 , 1 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.5304, 0.0051, 0.0024, 0.0051, 0.0057, 0.0051, 0.4461])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 1 , 1 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([3.8514e-03, 4.2793e-04, 9.9110e-01, 2.9487e-03, 1.5045e-03, 1.0698e-04,\n",
            "        6.0178e-05])\n",
            "then action 2 was taken\n",
            "first the state was:\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 1 , 1 , 1 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([0.1575, 0.2546, 0.1443, 0.0836, 0.1400, 0.1157, 0.1044])\n",
            "then action 0 was taken\n",
            "first the state was:\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "2 , 0 , 1 , 1 , 1 , 0 , 0\n",
            "--------------------\n",
            "[1, 1, 1, 1, 1, 1, 1]\n",
            "action ditribution is tensor([1.0551e-03, 5.0145e-01, 1.6882e-04, 2.6379e-04, 1.6882e-04, 4.9686e-01,\n",
            "        4.2206e-05])\n",
            "then action 5 was taken\n",
            "saving model into: alpha_four version: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Playing against it"
      ],
      "metadata": {
        "id": "I4bVXfY45u5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_against_comp(num_simulations_per_turn, tau, env, model):\n",
        "    count = 0\n",
        "    root_state = env.reset()\n",
        "    current_node = MCTSNode(root_state, env.get_action_mask(state[0]), False, env, model)\n",
        "\n",
        "    to_start = int(input(\"do you want to start? y/n\") == 'n')\n",
        "\n",
        "    while not env.is_terminal(current_node.state):\n",
        "        env.print_board(current_node.state)\n",
        "\n",
        "        if count % 2 == to_start:\n",
        "            action = int(input(\"pls enter your move:\")) - 1\n",
        "            delete_edges_except(current_node, action)\n",
        "            edge = next((edge for edge in current_node.edges if edge.action == action), -1)\n",
        "            if edge.node_after:\n",
        "                current_node = edge.node_after\n",
        "            else:\n",
        "                current_node = edge.open_edge(current_node.state, env, model)\n",
        "\n",
        "        else:\n",
        "            run_n_simulations(current_node, num_simulations_per_turn - current_node.visit_count(), env, model)\n",
        "            policy_dist = current_node.extract_policy(tau)\n",
        "            action = torch.multinomial(policy_dist, num_samples=1).item()\n",
        "            delete_edges_except(current_node, action)\n",
        "            edge = next((edge for edge in current_node.edges if edge.action == action), -1)\n",
        "            current_node = edge.node_after\n",
        "\n",
        "        print(f\"then action {action} was taken\")\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    if count % 2 == to_start:\n",
        "        print(\"you lost :(\")\n",
        "    else:\n",
        "        print(\"you won :)\")\n",
        "\n",
        "    return current_node"
      ],
      "metadata": {
        "id": "ScqL6OpX5hnE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = EffConnect4\n",
        "model = AlphaFourModel(config['num_blocks'], config['hidden_size']).to(config['device'])\n",
        "optimizer = optim.Adam(model.parameters(), lr = config['lr'])\n",
        "\n",
        "model.load(optimizer, \"alpha_four\", version=8)\n",
        "\n",
        "end_node = play_against_comp(1600, 2, env, model)\n",
        "env.print_board(end_node.state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYy_Q-cJEoBp",
        "outputId": "d4880171-4f92-42cd-bb65-b950339446ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-e8c2f28a1315>:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(os.path.join(config['saving_dir'], file_name + '_model_' + f'_version_{version}')))\n",
            "<ipython-input-6-e8c2f28a1315>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  optimizer.load_state_dict(torch.load(os.path.join(config['saving_dir'], file_name + '_optimizer_' + f'_version_{version}')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model and optimizer from: alpha_four version: 8\n",
            "do you want to start? y/nn\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "then action 4 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "--------------------\n",
            "pls enter your move:4\n",
            "then action 3 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "--------------------\n",
            "then action 4 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "--------------------\n",
            "pls enter your move:4\n",
            "then action 3 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "--------------------\n",
            "then action 3 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "--------------------\n",
            "pls enter your move:6\n",
            "then action 5 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 3 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "pls enter your move:5\n",
            "then action 4 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 3 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "pls enter your move:4\n",
            "then action 3 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 5 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 1 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "pls enter your move:3\n",
            "then action 2 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 1 , 0\n",
            "0 , 0 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 2 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 1 , 2 , 1 , 1 , 0\n",
            "0 , 0 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "pls enter your move:2\n",
            "then action 1 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 1 , 2 , 1 , 1 , 0\n",
            "0 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 0 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "pls enter your move:3\n",
            "then action 2 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 6 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 1\n",
            "--------------------\n",
            "pls enter your move:1\n",
            "then action 0 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 2 , 1 , 2 , 0 , 0\n",
            "2 , 0 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 1\n",
            "--------------------\n",
            "then action 6 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 2 , 1 , 2 , 0 , 0\n",
            "2 , 0 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 1\n",
            "--------------------\n",
            "pls enter your move:7\n",
            "then action 6 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 2 , 1 , 2 , 0 , 2\n",
            "2 , 0 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 1\n",
            "--------------------\n",
            "then action 6 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 1\n",
            "0 , 0 , 2 , 1 , 2 , 0 , 2\n",
            "2 , 0 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 1\n",
            "--------------------\n",
            "pls enter your move:7\n",
            "then action 6 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 1\n",
            "0 , 0 , 2 , 1 , 2 , 0 , 2\n",
            "2 , 0 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 1\n",
            "--------------------\n",
            "then action 0 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 1\n",
            "1 , 0 , 2 , 1 , 2 , 0 , 2\n",
            "2 , 0 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 1\n",
            "--------------------\n",
            "pls enter your move:7\n",
            "then action 6 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 2\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 1\n",
            "1 , 0 , 2 , 1 , 2 , 0 , 2\n",
            "2 , 0 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 1\n",
            "--------------------\n",
            "then action 4 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 2\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 0 , 0 , 1 , 1 , 0 , 1\n",
            "1 , 0 , 2 , 1 , 2 , 0 , 2\n",
            "2 , 0 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 1\n",
            "--------------------\n",
            "pls enter your move:6\n",
            "then action 5 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 2\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 0 , 0 , 1 , 1 , 0 , 1\n",
            "1 , 0 , 2 , 1 , 2 , 2 , 2\n",
            "2 , 0 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 1\n",
            "--------------------\n",
            "then action 5 was taken\n",
            "you lost :(\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 2\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 0 , 0 , 1 , 1 , 1 , 1\n",
            "1 , 0 , 2 , 1 , 2 , 2 , 2\n",
            "2 , 0 , 1 , 2 , 1 , 1 , 1\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 1\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Games I played against it"
      ],
      "metadata": {
        "id": "O70GpsdNfhfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end_node = play_against_comp(1600, 2, env, model)\n",
        "env.print_board(end_node.state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxJeO1g4AHHS",
        "outputId": "b41ffd19-dd77-421b-aae3-687e4d866874"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do you want to start? y/nn\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "then action 4 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "--------------------\n",
            "pls enter your move:4\n",
            "then action 3 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "--------------------\n",
            "then action 4 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "--------------------\n",
            "pls enter your move:4\n",
            "then action 3 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "--------------------\n",
            "then action 3 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "--------------------\n",
            "pls enter your move:6\n",
            "then action 5 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 3 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "pls enter your move:5\n",
            "then action 4 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 3 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "pls enter your move:4\n",
            "then action 3 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 5 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 1 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "pls enter your move:3\n",
            "then action 2 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 1 , 1 , 0\n",
            "0 , 0 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 2 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 1 , 2 , 1 , 1 , 0\n",
            "0 , 0 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "pls enter your move:2\n",
            "then action 1 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 1 , 2 , 1 , 1 , 0\n",
            "0 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 0 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 0 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "pls enter your move:2\n",
            "then action 1 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 1 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 1 , 2 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "pls enter your move:3\n",
            "then action 2 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 1 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 2 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 1 , 1 , 0 , 0 , 0\n",
            "0 , 1 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "pls enter your move:2\n",
            "then action 1 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 1 , 0 , 0 , 0\n",
            "0 , 1 , 2 , 1 , 2 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n",
            "then action 0 was taken\n",
            "you lost :(\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 1 , 0 , 0 , 0\n",
            "0 , 1 , 2 , 1 , 2 , 0 , 0\n",
            "1 , 2 , 1 , 2 , 1 , 1 , 0\n",
            "1 , 2 , 2 , 2 , 1 , 2 , 0\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end_node = play_against_comp(1600, 2, env, model)\n",
        "env.print_board(end_node.state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2d1b7Zs7AVQ",
        "outputId": "0134e802-aca2-414e-d421-1241c541eb5e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do you want to start? y/ny\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "--------------------\n",
            "pls enter your move:4\n",
            "then action 3 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "--------------------\n",
            "then action 6 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "pls enter your move:4\n",
            "then action 3 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "then action 1 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "pls enter your move:4\n",
            "then action 3 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "then action 3 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "pls enter your move:2\n",
            "then action 1 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "then action 6 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "pls enter your move:7\n",
            "then action 6 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 1 , 0 , 0 , 1\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "then action 1 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 1\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "pls enter your move:2\n",
            "then action 1 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 1\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "then action 2 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 1\n",
            "0 , 1 , 0 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "pls enter your move:3\n",
            "then action 2 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 1 , 0 , 0 , 1\n",
            "0 , 1 , 1 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "then action 2 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 0 , 2 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 1\n",
            "0 , 1 , 1 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "pls enter your move:3\n",
            "then action 2 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 1\n",
            "0 , 1 , 1 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "then action 1 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 1\n",
            "0 , 1 , 1 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "pls enter your move:3\n",
            "then action 2 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 1\n",
            "0 , 1 , 1 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 2\n",
            "--------------------\n",
            "then action 5 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 0 , 0 , 0\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 1\n",
            "0 , 1 , 1 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 0 , 2 , 2\n",
            "--------------------\n",
            "pls enter your move:7\n",
            "then action 6 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 0 , 0 , 1\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 1\n",
            "0 , 1 , 1 , 1 , 0 , 0 , 2\n",
            "0 , 2 , 2 , 1 , 0 , 2 , 2\n",
            "--------------------\n",
            "then action 5 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 0 , 0 , 1\n",
            "0 , 2 , 2 , 1 , 0 , 0 , 1\n",
            "0 , 1 , 1 , 1 , 0 , 2 , 2\n",
            "0 , 2 , 2 , 1 , 0 , 2 , 2\n",
            "--------------------\n",
            "pls enter your move:6\n",
            "then action 5 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 0 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 0 , 0 , 1\n",
            "0 , 2 , 2 , 1 , 0 , 1 , 1\n",
            "0 , 1 , 1 , 1 , 0 , 2 , 2\n",
            "0 , 2 , 2 , 1 , 0 , 2 , 2\n",
            "--------------------\n",
            "then action 2 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 0 , 0 , 1\n",
            "0 , 2 , 2 , 1 , 0 , 1 , 1\n",
            "0 , 1 , 1 , 1 , 0 , 2 , 2\n",
            "0 , 2 , 2 , 1 , 0 , 2 , 2\n",
            "--------------------\n",
            "pls enter your move:5\n",
            "then action 4 was taken\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 0 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 0 , 0 , 1\n",
            "0 , 2 , 2 , 1 , 0 , 1 , 1\n",
            "0 , 1 , 1 , 1 , 0 , 2 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 2 , 2\n",
            "--------------------\n",
            "then action 3 was taken\n",
            "Player 1 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 0 , 0 , 1\n",
            "0 , 2 , 2 , 1 , 0 , 1 , 1\n",
            "0 , 1 , 1 , 1 , 0 , 2 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 2 , 2\n",
            "--------------------\n",
            "pls enter your move:5\n",
            "then action 4 was taken\n",
            "you won :)\n",
            "Player 2 to play:\n",
            "board:\n",
            "--------------------\n",
            "0 , 0 , 2 , 0 , 0 , 0 , 0\n",
            "0 , 2 , 1 , 2 , 0 , 0 , 0\n",
            "0 , 1 , 1 , 2 , 0 , 0 , 1\n",
            "0 , 2 , 2 , 1 , 0 , 1 , 1\n",
            "0 , 1 , 1 , 1 , 1 , 2 , 2\n",
            "0 , 2 , 2 , 1 , 1 , 2 , 2\n",
            "--------------------\n"
          ]
        }
      ]
    }
  ]
}
